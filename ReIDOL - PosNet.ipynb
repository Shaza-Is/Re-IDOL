{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM,Concatenate, Dense, Flatten ,Activation ,Input , BatchNormalization,Dropout , Bidirectional\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "building1_filenames = set(glob.glob(\"datasets/building1/*/*.feather\"))\n",
    "building2_filenames = set(glob.glob(\"datasets/building2/*/*.feather\"))\n",
    "building3_filenames = set(glob.glob(\"datasets/building3/*/*.feather\"))\n",
    "training_filenames = set(glob.glob(\"datasets/building1/train/*.feather\"))\n",
    "test1_filenames = building1_filenames - training_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed=10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training samples\n",
    "\n",
    "dfs = []\n",
    "for file in training_filenames:\n",
    "    df = pd.read_feather(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "training_data = pd.concat(dfs, ignore_index=True)\n",
    "training_data.columns\n",
    "#training_data.head()\n",
    "\n",
    "orientation_acc = training_data[[\"iphoneAccX\", \"iphoneAccY\", \"iphoneAccZ\"]]\n",
    "#orientation_acc.drop([0], inplace=True)\n",
    "acc_np = orientation_acc.to_numpy()\n",
    "orientation_gyro = training_data[[ \"iphoneGyroX\", \"iphoneGyroY\", \"iphoneGyroZ\"]]\n",
    "#orientation_gyro.drop([0], inplace=True)\n",
    "gyro_np = orientation_gyro.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_np[:100,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2087464"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_target = training_data[[\"processedPosX\", \"processedPosY\" ]]#, \"processedPosZ\"]] \n",
    "\n",
    "position_diff_target = position_target.diff()\n",
    "position_diff_target.drop([0], inplace=True)\n",
    "position_diff_target.head()\n",
    "\n",
    "pos_np = position_target.to_numpy()\n",
    "len(pos_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_samples(acc_, gyro_, pos_, batch_size = 64):    \n",
    "    while True:\n",
    "        xa_batch = np.zeros((batch_size,100,3))\n",
    "        xg_batch = np.zeros((batch_size,100,3))\n",
    "        y_batch = np.zeros((batch_size,2))\n",
    "        j = 0\n",
    "        for i in range(len(pos_)):\n",
    "            xa_batch[j,:,:] = acc_[i, :]\n",
    "            xg_batch[j,:,:] = gyro_[i,:]\n",
    "            y_batch[j,:] = pos_[i,:]\n",
    "            j += 1\n",
    "            if j >= batch_size:\n",
    "                j = 0              \n",
    "                yield([xa_batch,xg_batch],y_batch)\n",
    "            #---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##gyro = ['iphoneGyroX', 'iphoneGyroY', 'iphoneGyroZ']\n",
    "##acc = [\"iphoneAccX\", \"iphoneAccY\", \"iphoneAccZ\"]\n",
    "##pos = [\"processedPosX\", \"processedPosY\"]#, \"processedPosZ\"]\n",
    "##\n",
    "##in_dataset_2 = (\n",
    "##    tf.data.Dataset.from_tensor_slices(\n",
    "##        (\n",
    "##            tf.cast(orientation_gyro[gyro].values, tf.float32)\n",
    "##        )\n",
    "##    )\n",
    "##)\n",
    "##\n",
    "##in_dataset_1 = (\n",
    "##    tf.data.Dataset.from_tensor_slices(\n",
    "##        (\n",
    "##            \n",
    "##            tf.cast(orientation_acc[acc].values, tf.float32)\n",
    "##        )\n",
    "##    )\n",
    "##)\n",
    "##\n",
    "##out_dataset = (tf.data.Dataset.from_tensor_slices((tf.cast(position_diff_target[pos].values, tf.int32))))\n",
    "##\n",
    "##\n",
    "### Hack -> open TF issue: https://github.com/tensorflow/tensorflow/issues/44046\n",
    "##in_dataset_1 =in_dataset_1.window(100,1,1)\n",
    "##in_dataset_1 = in_dataset_1.flat_map(lambda x:x).batch(100).batch(64) #flat map but do nothing to values, then batch twice to get what model.fit expects from windowed datasets\n",
    "##in_dataset_2 =in_dataset_2.window(100,1,1)\n",
    "##in_dataset_2 = in_dataset_2.flat_map(lambda x:x).batch(100).batch(64) #flat map but do nothing to values, then batch twice to get what model.fit expects from windowed datasets\n",
    "##\n",
    "##out_dataset = out_dataset.window(100,1,1)\n",
    "##out_dataset = out_dataset.flat_map(lambda x:x).batch(100).batch(64)\n",
    "##in_dataset =  tf.data.Dataset.zip(((in_dataset_1, in_dataset_2), out_dataset))\n",
    "###training_dataset = tf.data.Dataset.zip((in_dataset, out_dataset))\n",
    "###seq_training_dataset = training_dataset.window(100, shift=1)\n",
    "###windows = next(iter(seq_training_dataset))\n",
    "#ds = training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = os.path.join(os.getcwd(),'datasets/CheckPoints_Model_PostNet')\n",
    "\n",
    "if not os.path.exists(checkpoints_path):\n",
    "  os.mkdir(checkpoints_path)\n",
    "check_point_template_path = os.path.join(checkpoints_path,'ckpt_epoch_{epoch:03d}_loss_{loss:.4f}_vloss_{val_loss:.4f}.hdf5')\n",
    "check_point_callback = tf.keras.callbacks.ModelCheckpoint(check_point_template_path)\n",
    "\n",
    "import re #regular expresion\n",
    "def get_all_checkpoints(checkpoints_path,checkpoint_main_name = 'ckpt'):\n",
    "  all_checkpoints = [j for j in os.listdir(checkpoints_path) if j.startswith(checkpoint_main_name)]\n",
    "  return all_checkpoints\n",
    "\n",
    "def check_if_available_checkpoints(checkpoints_path,checkpoint_main_name = 'ckpt'):\n",
    "  all_checkpoints = get_all_checkpoints(checkpoints_path,checkpoint_main_name)\n",
    "  if(len(all_checkpoints) > 0):#checkpoints avilable\n",
    "    all_checkpoints.sort(reverse=True)    \n",
    "    latest_check_point = all_checkpoints[0]\n",
    "    initial_epoch = int(re.search('epoch_(.*?)_', latest_check_point).group(1))    \n",
    "  else:\n",
    "    latest_check_point = None\n",
    "    initial_epoch = 0\n",
    "    \n",
    "  return initial_epoch , latest_check_point\n",
    "\n",
    "\n",
    "# Check if there are any check points initially\n",
    "initial_epoch , latest_check_point = check_if_available_checkpoints(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    # Building Network\n",
    "    #1. Define inputs\n",
    "    Acc_input = Input(shape=(100,3) , name = 'Acc_input')    \n",
    "    Gyro_input = Input(shape=(100,3) , name = 'Gyro_input')\n",
    "    \n",
    "    MergedLayer = Concatenate()([Acc_input , Gyro_input])\n",
    "    #\n",
    "    ##MergedLayer = Input(shape=(9) , name = 'imu_input')Bidirectional(\n",
    "    #2. LSTM\n",
    "    LSTM1 = (Bidirectional(LSTM(100, return_sequences=True)))(MergedLayer) # , return_sequences=True\n",
    "    LSTM2 = (Bidirectional(LSTM(100)))(LSTM1)\n",
    "    \n",
    "\n",
    "    \n",
    "    #4. Fully-Connected (x)\n",
    "    Dense3 = Dense(units=100, activation='tanh')(LSTM2)\n",
    "    Dense4 = Dense(units=20, activation='tanh')(Dense3)\n",
    "    x_output = Dense(units=2, activation='linear')(Dense4) # Theta Outputs\n",
    "    \n",
    "    #5. Define and compile The model\n",
    "    Network = Model([Acc_input,Gyro_input], [x_output]) # , sigma_output])  #  [Acc_input,Gyro_input, Mag_input]\n",
    "    Network.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=pos_learning_rate) ,metrics=['mape']) #loss=OreintLoss()\n",
    "    return Network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 11:33:31.353900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.359176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.359439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.359992: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-24 11:33:31.360498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.360682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.360858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.717145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.717491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.717700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-24 11:33:31.717893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4297 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Acc_input (InputLayer)          [(None, 100, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Gyro_input (InputLayer)         [(None, 100, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 6)       0           Acc_input[0][0]                  \n",
      "                                                                 Gyro_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 200)     85600       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          240800      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          20100       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           2020        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            42          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 348,562\n",
      "Trainable params: 348,562\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#network_builder = PosNet()\n",
    "position_network = build_model()\n",
    "position_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 11:33:32.719226: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 11:33:35.834440: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 113340/Unknown - 2113s 19ms/step - loss: 46.9327 - mape: 11715.5869"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12047/1922903861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = position_network.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mgenerate_training_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgyro_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Re-IDOL/.venv/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Re-IDOL/.venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Re-IDOL/.venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Re-IDOL/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Re-IDOL/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Desktop/Re-IDOL/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Re-IDOL/.venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Nepochs = 20\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((acc_np, gyro_np)).window(100,1,1)\n",
    "out_data = tf.data.Dataset.from_tensor_slices((pos_np)).window(100,1,1)\n",
    "dataset =  tf.data.Dataset.zip((train_dataset, out_data))\n",
    "\n",
    "\n",
    "history = position_network.fit(\n",
    "    generate_training_samples(acc_np, gyro_np, pos_np),\n",
    "    epochs=Nepochs,\n",
    "    initial_epoch=initial_epoch\n",
    "    #callbacks = [check_point_callback]\n",
    ")\n",
    "\n",
    "#     [orientation_acc, \n",
    "#    orientation_gyro],\n",
    "#    position_diff_target,"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "247724489718390394f02e528248e3d22e0afb8fd8f14b8b8ec22f047ea12946"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
